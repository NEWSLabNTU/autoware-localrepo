# CLAUDE.md

This file provides guidance to Claude Code when working with this repository.

## Repository Overview

This project builds Debian packages for multiple Autoware versions and creates a local APT repository. It combines:
- **ROS packages** built via colcon2deb (in Docker)
- **Meta-packages** built with debhelper (autoware-config, autoware-theme, autoware-data, autoware-runtime, autoware-full)

## Directory Structure

```
autoware-localrepo/
├── common/autoware-localrepo/    # APT repo configuration package
├── 1.5.0/                        # Autoware 1.5.0 version
│   ├── amd64/                    # colcon2deb build for x86_64
│   ├── jp62/                     # colcon2deb build for JetPack 6.2 (arm64)
│   ├── packages/                 # Debhelper packages
│   │   ├── autoware-config/      # CycloneDDS config, env setup
│   │   ├── autoware-theme/       # RViz theme/icons (uses genpkg.py)
│   │   ├── autoware-data/        # ML models (uses genpkg.py)
│   │   ├── autoware-runtime/     # Meta-package for ROS debs
│   │   └── autoware-full/        # Complete install meta-package
│   ├── output/                   # Consolidated .deb files
│   └── justfile
├── 2025.02/                      # Autoware 2025.02 version
│   ├── amd64/                    # colcon2deb build for x86_64
│   ├── jp60/                     # colcon2deb build for JetPack 6.0 (arm64)
│   └── ...
├── repo/                         # Final APT repository
└── justfile                      # Top-level build automation
```

### JetPack Directory Naming

Jetson builds use JetPack version suffixes instead of architecture:
- `jp60/` - JetPack 6.0 (L4T R36.3, CUDA 12.2)
- `jp62/` - JetPack 6.2 (L4T R36.4, CUDA 12.6)

## Key Commands

### Build Meta-Packages (autoware-config, autoware-theme, etc.)

```bash
# From version directory (e.g., 2025.02/)
just build-packages

# Or individually
cd packages/autoware-config && dpkg-buildpackage -us -uc -b
```

### Build ROS Packages (requires Docker + colcon2deb)

```bash
# x86_64 build
cd 1.5.0/amd64
just build

# ARM64/Jetson build (requires QEMU on x86 host - very slow)
cd 1.5.0/jp62
just build

# Monitor build progress (arm64 builds take several hours)
tail -f build/log/latest/*colcon_build.log
```

### Generate Package Files (autoware-data, autoware-theme)

These packages use `genpkg.py` scripts to generate debian/ files and download lists:

```bash
# Regenerate autoware-data debian files
cd packages/autoware-data
python3 genpkg.py --version 2025.02

# Regenerate autoware-theme debian files
cd packages/autoware-theme
python3 genpkg.py --version 2025.02
```

### Create APT Repository

```bash
# From repo root
just create-repo
```

## Package Types

| Package            | Arch | Description                              |
|--------------------|------|------------------------------------------|
| autoware-config    | all  | CycloneDDS config, environment setup     |
| autoware-theme     | all  | RViz icons and Qt theme (from GitHub)    |
| autoware-data      | all  | ML model files (ONNX, etc. from GitHub)  |
| autoware-runtime   | any  | Meta-package depending on ROS packages   |
| autoware-full      | any  | Complete Autoware installation           |
| autoware-localrepo | all  | APT repository configuration             |

## Important Files

### Generated Files (track in git)

These files are generated by genpkg.py but should be committed:
- `packages/autoware-data/downloads.txt` - aria2c download list for ML models
- `packages/autoware-data/tasks.yaml` - Downloaded from Autoware repo
- `packages/autoware-theme/downloads.txt` - aria2c download list for theme files
- `*/amd64/rosdep-packages.txt` - List of rosdep-resolved packages

### Jetson Build Files

Files specific to Jetson/ARM64 builds in `*/jp*/` directories:
- `cuda_init.cmake` - Ensures CUDA is found early for cudnn_cmake_module compatibility
- `opencv-preferences` - APT pinning to prefer Ubuntu OpenCV over L4T version
- `rosdep-packages.txt` - ARM64-specific rosdep packages (some differ from amd64)

### Debhelper Packaging

Each package in `packages/` contains standard debian/ directory:
- `debian/control` - Package metadata and dependencies
- `debian/rules` - Build instructions (uses dh)
- `debian/changelog` - Version history
- `debian/compat` or `debian/debhelper-compat` - Debhelper version

### genpkg.py Scripts

`autoware-data/genpkg.py` and `autoware-theme/genpkg.py`:
- Download metadata from Autoware GitHub repository
- Generate `debian/rules` with aria2c parallel download commands
- Generate `downloads.txt` for aria2c input
- Use `$(CURDIR)/debian/<package>/` for install paths (not `$(DESTDIR)`)

## Common Development Tasks

### Adding a New Autoware Version

1. Copy existing version directory structure
2. Update version in `packages/*/debian/changelog`
3. Regenerate package files: `python3 genpkg.py --version <new-version>`
4. Update justfile if needed

### Modifying Package Dependencies

Edit `debian/control` in the respective package directory. For autoware-runtime and autoware-full, update the `Depends:` field.

### Debugging Build Failures

```bash
# Check build logs
cat packages/autoware-config/debian/autoware-config/*.log

# Build with verbose output
DH_VERBOSE=1 dpkg-buildpackage -us -uc -b
```

### Cleaning Build Cache for Specific Packages

To rebuild specific packages after fixing issues, clean their CMake cache inside the container:
```bash
# Clean specific package build cache (run inside container to avoid permission issues)
docker run --rm --platform linux/arm64 \
  -v "$(pwd)/build:/output" IMAGE_NAME \
  rm -rf /output/sources/build/PACKAGE_NAME

# Then rebuild with skip flags to speed up
colcon2deb --workspace source --config config.yaml \
  --skip-rosdep-install --skip-copy-src --skip-gen-rosdep-list
```

## Known Issues

### Makefile Heredoc Syntax

Don't use heredoc (`<< 'EOF'`) in debian/rules - it causes "missing separator" errors. Write content to files separately instead.

### Install Path in debian/rules

Always use `$(CURDIR)/debian/<package-name>/` for install destinations, not `$(DESTDIR)/`. Example:
```makefile
install -d $(CURDIR)/debian/autoware-config/opt/autoware/config
```

## Jetson/ARM64 Build Issues

### Multi-Arch Setup (QEMU on x86 Host)

Building ARM64 Docker images on an x86_64 host requires QEMU user-mode emulation. This is a one-time setup.

**Install QEMU:**
```bash
sudo apt install qemu-user-static
```

**Register QEMU with Docker (with credential support for setuid binaries):**
```bash
docker run --rm --privileged multiarch/qemu-user-static --reset -p yes --credential yes
```

The `--credential yes` flag is critical - it sets binfmt flags to `OCF` which allows setuid binaries like `sudo` to work properly inside emulated containers. Without it, you'll see:
```
sudo: effective uid is not 0, is /usr/bin/sudo on a file system with the 'nosuid' option set...
```

**Verify registration:**
```bash
cat /proc/sys/fs/binfmt_misc/qemu-aarch64
# Should show: flags: OCF
```

**Note:** The registration persists across reboots on most systems, but may need to be re-run after kernel updates or Docker daemon restarts.

### Cross-Compilation Platform Flag

When building arm64 Docker images on amd64 host, `--platform linux/arm64` is required. The `config.yaml` must include:
```yaml
docker:
  platform: linux/arm64
```

### OpenCV Version Conflict (L4T vs Ubuntu)

NVIDIA L4T base images have OpenCV 4.8.0 pre-installed (NOT via APT). Ubuntu provides 4.5.4. This causes CMake conflicts:
```
The imported target "opencv_core" references the file "/usr/lib/libopencv_core.so.4.8.0" but this file does not exist.
```

**Solution in Dockerfile**: Remove L4T OpenCV files and use APT preferences to force Ubuntu's version:
```dockerfile
RUN rm -rf /usr/lib/libopencv* /usr/lib/cmake/opencv4 /usr/include/opencv4 \
           /usr/local/lib/libopencv* /usr/local/lib/cmake/opencv4 /usr/local/include/opencv4 \
           /usr/share/OpenCV /usr/share/opencv4 /opt/opencv*
COPY opencv-preferences /etc/apt/preferences.d/opencv-preferences
RUN apt update && apt install -y libopencv-dev
```

The `opencv-preferences` file contains APT pinning rules to prefer Ubuntu's OpenCV packages.

### L4T Base Image Has Old CMake

L4T base images ship with CMake 3.14.4 in `/usr/local/bin`, which is too old for Autoware (requires 3.16+).

**Solution**: Remove the old CMake and install from Ubuntu repos:
```dockerfile
RUN rm -f /usr/local/bin/cmake /usr/local/bin/ctest /usr/local/bin/cpack /usr/local/bin/ccmake
RUN apt update && apt install -y cmake
```

Ubuntu 22.04 provides CMake 3.22.1, which:
- Meets Autoware's minimum requirement (3.16+)
- Has working FindCUDA module (deprecated in 3.27+, causes issues in 3.28+)
- Avoids CMake 4.x compatibility issues with older ROS packages

### python3-torch Not Available via APT

`python3-torch` is not in Ubuntu repos for arm64. Comment it out in `rosdep-packages.txt`:
```
# python3-torch  # Not available via apt on arm64/Jetson
```

PyTorch must be installed separately on target using NVIDIA Jetson wheels.

### Docker Volumes and sudo (nosuid)

Docker volume mounts use `nosuid` by default. The colcon2deb helper scripts use `run_privileged()` to handle this - running commands directly when uid=0 instead of using sudo.

### CMake Can't Find CUDA

CMake needs help finding CUDA on L4T images. Add CUDA environment variables to the Dockerfile:
```dockerfile
ENV CUDA_HOME=/usr/local/cuda
ENV CMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc
ENV PATH=/usr/local/cuda/bin:${PATH}
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH}
```

### cuda_init.cmake for CUDNN Detection

The ROS `cudnn_cmake_module` provides `FindCUDNN.cmake` that requires `CUDA_FOUND` and `find_cuda_helper_libs` macro to be already defined. However, it doesn't call `find_package(CUDA)` first. This causes errors like:

```
Could NOT find CUDNN (missing: CUDNN_LIBRARY CUDNN_INCLUDE_DIR)
```

**Solution**: Use `CMAKE_PROJECT_INCLUDE` to inject CUDA initialization early:

```cmake
# cuda_init.cmake - validates nvcc before calling find_package(CUDA)
if(NOT CUDA_FOUND)
  find_program(_CUDA_NVCC_EXECUTABLE nvcc
    HINTS ENV CUDA_PATH ENV CUDA_HOME
    PATH_SUFFIXES bin
    PATHS /usr/local/cuda/bin
  )

  if(_CUDA_NVCC_EXECUTABLE)
    execute_process(
      COMMAND "${_CUDA_NVCC_EXECUTABLE}" --version
      OUTPUT_VARIABLE _nvcc_version_output
      ERROR_QUIET
      RESULT_VARIABLE _nvcc_result
    )

    # Only call find_package(CUDA) if nvcc output matches expected format
    # FindCUDA.cmake expects "release X.Y" in the version output
    if(_nvcc_result EQUAL 0 AND _nvcc_version_output MATCHES "release [0-9]+\\.[0-9]+")
      find_package(CUDA QUIET)
    endif()
  endif()
endif()
```

**Important**: The nvcc output validation is critical under QEMU emulation. Without it, CMake's `FindCUDA.cmake` may fail with:
```
CMake Error at /usr/share/cmake-3.22/Modules/FindCUDA.cmake:929 (string):
  string sub-command REGEX, mode REPLACE needs at least 6 arguments
```

Configure via `/colcon2deb-setup.sh`:
```dockerfile
COPY cuda_init.cmake /cuda_init.cmake
RUN echo 'export COLCON2DEB_CMAKE_ARGS="-DCMAKE_PROJECT_INCLUDE=/cuda_init.cmake"' >> /colcon2deb-setup.sh
```

### CUDA Architecture Detection Fails in QEMU

When building arm64 Docker images under QEMU emulation, CUDA architecture detection fails:
```
nvcc fatal : Unsupported gpu architecture 'compute_native'
```

This happens because `CUDA_ARCHITECTURES native` tries to detect the GPU but QEMU doesn't expose one.

**Solution**: Set explicit architecture via environment variable in Dockerfile:
```dockerfile
ENV CUDAARCHS=87  # Orin for JetPack 6.x
```

### ASLR Causes Segfaults in QEMU (Kernel ≥6.8.0-50)

On Linux kernels ≥6.8.0-50, QEMU user-mode emulation has a known incompatibility with ASLR (Address Space Layout Randomization) that causes random segmentation faults during C++ compilation:

```
c++: internal compiler error: Segmentation fault signal terminated program cc1plus
```

**Solution**: Temporarily disable ASLR on the host during the build:
```bash
# Disable ASLR before build
sudo sysctl kernel.randomize_va_space=0

# Run the build
just ros

# Re-enable ASLR after build completes
sudo sysctl kernel.randomize_va_space=2
```

**Security note:** Disabling ASLR reduces system security. Only disable it temporarily during builds and re-enable immediately after. For security-critical environments, consider building on native ARM64 hardware instead.

**References:**
- https://github.com/docker/buildx/issues/3170
- https://gitlab.com/qemu-project/qemu/-/issues/1325

### LTO Conflicts with CUDA (fatbinData Symbol)

Ubuntu 22.04 enables LTO (Link Time Optimization) by default via `dpkg-buildflags`. This conflicts with CUDA's fatbin symbols during linking:

```
Error: symbol `fatbinData' is already defined
lto-wrapper: fatal error: make returned 2 exit status
```

**Solution**: Disable LTO in `debian/rules` for CUDA packages only. Add to `debian-overrides/<cuda-package>/debian/rules`:

```makefile
# Disable LTO - causes conflicts with CUDA fatbin symbols
export DEB_BUILD_MAINT_OPTIONS = hardening=+all reproducible=+fixfilepath optimize=-lto
```

CUDA packages requiring this fix (24 packages):
- autoware_bevfusion, autoware_calibration_status_classifier, autoware_camera_streampetr
- autoware_cuda_pointcloud_preprocessor, autoware_cuda_utils, autoware_diffusion_planner
- autoware_image_projection_based_fusion, autoware_lidar_centerpoint, autoware_lidar_frnet
- autoware_lidar_transfusion, autoware_probabilistic_occupancy_grid_map, autoware_ptv3
- autoware_shape_estimation, autoware_simpl_prediction, autoware_tensorrt_bevformer
- autoware_tensorrt_classifier, autoware_tensorrt_common, autoware_tensorrt_plugins
- autoware_tensorrt_yolox, autoware_traffic_light_classifier, autoware_traffic_light_fine_detector
- bevdet_vendor, cuda_blackboard, trt_batched_nms

### spconv/cumm Required for Perception Packages

`autoware_tensorrt_plugins` requires spconv (sparse convolution) and cumm libraries for BEVFusion and other perception models:
```
Target "autoware_tensorrt_plugins" links to: spconv::spconv but the target was not found.
```

**Solution**: Install pre-built packages from autowarefoundation/spconv_cpp:
```dockerfile
RUN wget -q https://github.com/autowarefoundation/spconv_cpp/releases/download/spconv_v2.3.8%2Bcumm_v0.5.3%2Bcu128/cumm_0.5.3_arm64-jetson.deb && \
    wget -q https://github.com/autowarefoundation/spconv_cpp/releases/download/spconv_v2.3.8%2Bcumm_v0.5.3%2Bcu128/spconv_2.3.8_arm64-jetson.deb && \
    apt-get install -y ./cumm_0.5.3_arm64-jetson.deb ./spconv_2.3.8_arm64-jetson.deb && \
    rm -f cumm_0.5.3_arm64-jetson.deb spconv_2.3.8_arm64-jetson.deb
```

### Build Directory Permission Issues

If you see `Permission denied` errors when trying to `rm -rf build/`:
```
rm: cannot remove 'build/sources/src': Permission denied
```

This indicates that Docker created files owned by root in the mounted build directory.

**Workaround**: Use `sudo rm -rf build/` to clean up, then restart the build.

**Better approach**: Run rm commands inside the container to avoid permission issues:
```bash
docker run --rm --platform linux/arm64 -v "$(pwd)/build:/output" IMAGE_NAME rm -rf /output/sources/build/PACKAGE_NAME
```

**Root cause**: The build runs as root inside the container. The colcon2deb `entry.sh` script uses a `trap` handler to run `chown -R` on exit (success, failure, or interrupt), but in rare cases files may still be left owned by root.

### Debhelper State Files Causing Build Phase Skips

If deb package builds fail with `.obj-aarch64-linux-gnu: No such file or directory`, it means debhelper skipped the configure/build phases because it found stale state files from a previous build.

**Root cause**: Files like `debian/debhelper-build-stamp` and `debian/.debhelper/` directory from previous builds make `dh` think the package is already configured/built.

**Solution**: colcon2deb now automatically cleans these files before copying debian directories. If using an older version, manually clean:
```bash
find build/build -name "debhelper-build-stamp" -delete
find build/build -path "*/debian/.debhelper" -type d -exec rm -rf {} +
```

## Development Practices

### Temporary Files

Always create temporary files in the project's `tmp/` directory instead of system `/tmp`:
```bash
mkdir -p tmp/
# Use tmp/ for intermediate build artifacts, logs, etc.
```

## Related Projects

- **colcon2deb** (`~/repos/colcon2deb`): Builds ROS packages into .deb files in Docker containers. Used by `*/amd64/` and `*/jp*/` directories. Modified to support `platform` config for cross-compilation. After modifying colcon2deb, reinstall: `cd ~/repos/colcon2deb && just build && pip install --user --force-reinstall dist/*.whl`

### colcon2deb Dockerfile Requirements

colcon2deb is distro-agnostic. If `/colcon2deb-setup.sh` exists in the Docker image, it will be sourced before building. This allows users to set up ROS environment or any other dependencies:

```dockerfile
# Simple ROS setup
RUN echo 'source /opt/ros/humble/setup.bash' > /colcon2deb-setup.sh

# Or for multiple sources / custom setup
COPY my-setup.sh /colcon2deb-setup.sh
```

This approach:
- Optional - works without the script
- Flexible - source anything you need
- Clean - no ENV clutter in Dockerfile
